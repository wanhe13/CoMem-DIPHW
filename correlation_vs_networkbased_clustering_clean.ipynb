{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ffefbd-b5d2-4066-b4ca-5d6777c91a56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/he.wan1/.conda/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-01 23:25:46.781430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-01 23:25:47.478767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/anaconda3/2022.05/lib:/shared/centos7/nodejs/14.15.4/lib\n",
      "2025-01-01 23:25:47.478831: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-01-01 23:25:52.500353: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/anaconda3/2022.05/lib:/shared/centos7/nodejs/14.15.4/lib\n",
      "2025-01-01 23:25:52.502855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/anaconda3/2022.05/lib:/shared/centos7/nodejs/14.15.4/lib\n",
      "2025-01-01 23:25:52.502891: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'wurlitzer', 'graph_tool', 'bayanpy', 'infomap'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t<script type=\"text/javascript\">\n",
       "\t\t\t<!--\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('script');\n",
       "\t\t\t\telement.type = 'text/javascript';\n",
       "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('style');\n",
       "\t\t\t\telement.type = 'text/css';\n",
       "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('div');\n",
       "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
       "\t\t\t\tdocument.body.appendChild(element);\n",
       "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
       "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t-->\n",
       "\t\t\t</script>\n",
       "\t\t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'ASLPAw', 'pyclustering'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'wurlitzer', 'infomap'}\n",
      "Generated Modules:\n",
      "Module 1: Rows 0-35, Columns 0-33\n",
      "Module 2: Rows 100-130, Columns 33-63\n",
      "Module 3: Rows 200-235, Columns 66-100\n",
      "Module 4: Rows 300-338, Columns 100-133\n",
      "Module 5: Rows 400-442, Columns 133-166\n",
      "Module 6: Rows 500-527, Columns 166-200\n",
      "Module 7: Rows 600-635, Columns 200-229\n",
      "Module 8: Rows 700-744, Columns 233-259\n",
      "Module 9: Rows 800-833, Columns 266-298\n",
      "Module 10: Rows 900-955, Columns 300-333\n",
      "Module 11: Rows 1000-1044, Columns 333-366\n",
      "Module 12: Rows 1100-1127, Columns 366-400\n",
      "Module 13: Rows 1200-1232, Columns 400-433\n",
      "Module 14: Rows 1300-1334, Columns 433-466\n",
      "Module 15: Rows 1400-1440, Columns 466-493\n",
      "Module 16: Rows 1500-1524, Columns 500-533\n",
      "Module 17: Rows 1600-1643, Columns 533-566\n",
      "Module 18: Rows 1700-1732, Columns 566-600\n",
      "Module 19: Rows 1800-1832, Columns 600-633\n",
      "Module 20: Rows 1900-1934, Columns 633-666\n",
      "Module 21: Rows 2000-2045, Columns 666-700\n",
      "Module 22: Rows 2100-2135, Columns 700-733\n",
      "Module 23: Rows 2200-2240, Columns 733-766\n",
      "Module 24: Rows 2300-2342, Columns 766-800\n",
      "Module 25: Rows 2400-2433, Columns 800-833\n",
      "Module 26: Rows 2500-2535, Columns 833-865\n",
      "Module 27: Rows 2600-2645, Columns 866-900\n",
      "Module 28: Rows 2700-2743, Columns 900-933\n",
      "Module 29: Rows 2800-2833, Columns 933-966\n",
      "Module 30: Rows 2900-2940, Columns 966-996\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import zscore\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import community\n",
    "import leidenalg as la\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n",
    "import csv\n",
    "from umap import UMAP\n",
    "import time\n",
    "from cdlib import algorithms\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from Simulation_functions import *\n",
    "from CoMemDIPHW import *\n",
    "from DIPHW import *\n",
    "from plot_functions import *\n",
    "\n",
    "parent_directory = os.path.abspath('..')\n",
    "sys.path.append(parent_directory)\n",
    "from functions import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder='ngenes'\n",
    "filename = f'ClusteringPerformance_{folder}_parallel.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rows=3000\n",
    "cols=1000\n",
    "num_modules=30\n",
    "avg_genes_per_module = 35\n",
    "avg_cells_per_module=38\n",
    "target_density= 0.03\n",
    "module_density=0.3\n",
    "inter_module_density=0.1\n",
    "inter_module_connection_probability=0.6\n",
    "lambda_background=10\n",
    "lambda_module=20\n",
    "inter_module_lambda=10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 1.1 Generate a Simulated Sparse Matrix (count data)\n",
    "# # 1.2 Normalise the sparse counts data\n",
    "# # 2.  Correlation-Based Graph Projection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modules = simulate_input_modules(rows, cols, num_modules, avg_genes_per_module,avg_cells_per_module)\n",
    "\n",
    "print(\"Generated Modules:\")\n",
    "for i, (start_row, end_row, start_col, end_col) in enumerate(modules):\n",
    "    print(f\"Module {i + 1}: Rows {start_row}-{end_row}, Columns {start_col}-{end_col}\")\n",
    "\n",
    "sparse_data = create_sparse_matrix_with_inter_module_variance(\n",
    "    rows, cols, modules, target_density, module_density, inter_module_density, \n",
    "    inter_module_connection_probability, lambda_background, lambda_module, inter_module_lambda\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_df_community = pd.DataFrame.sparse.from_spmatrix(sparse_data.T)\n",
    "community_assignments=modules_to_community_dict(modules, cols)\n",
    "data_df_community['community']=list(community_assignments.values())\n",
    "data_df_community=data_df_community.T\n",
    "print(sum([i=='NA' for i in list(community_assignments.values())]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "column_names = data_df_community.columns.tolist()\n",
    "\n",
    "shuffled_column_names = np.random.permutation(column_names)\n",
    "\n",
    "shuffled_df_community = data_df_community[shuffled_column_names]\n",
    "\n",
    "shuffled_sparse_data=sp.csr_matrix(shuffled_df_community.iloc[:-1].astype(np.float64))\n",
    "\n",
    "\n",
    "\n",
    "# # Shuffle the simulated scRNAseq matrix\n",
    "# # G_weighted_HT is a weighted graph but sparsified by HT\n",
    "# # G_weighted is a fully-connected weighted graph without sparsification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_sparsematrix_cpm(matrix):\n",
    "    \"\"\"\n",
    "    Normalize the matrix using Counts Per Million (CPM) normalization.\n",
    "\n",
    "    :param matrix: A sparse matrix\n",
    "    :return: Normalized matrix\n",
    "    \"\"\"\n",
    "    dense_matrix = matrix.toarray()\n",
    "    column_sums = np.sum(dense_matrix, axis=0)\n",
    "    column_sums[column_sums == 0] = 1\n",
    "    normalized_matrix = 1e6 * dense_matrix / column_sums\n",
    "    return sp.csr_matrix(normalized_matrix)\n",
    "\n",
    "\n",
    "\n",
    "def normalize_matrix_cpm(matrix):\n",
    "    \"\"\"\n",
    "    Normalize the matrix using Counts Per Million (CPM) normalization.\n",
    "\n",
    "    :param matrix: A sparse matrix\n",
    "    :return: Normalized matrix\n",
    "    \"\"\"\n",
    "    matrix=matrix.copy()\n",
    "    column_sums = np.sum(matrix, axis=0)\n",
    "    column_sums[column_sums == 0] = 1\n",
    "    normalized_matrix = 1e6 * matrix / column_sums\n",
    "    return normalized_matrix\n",
    "\n",
    "def filter_zero_expression_community(expression_df_with_CommunityAssignment,thres=0):\n",
    "    \"\"\"\n",
    "    Input: expression dataframe with the last row being the community assignment\n",
    "    Filter out genes (rows) and cells (columns) with zero expression.\n",
    "    :return: filtered expression dataframe with community assignment\n",
    "    \"\"\"\n",
    "    df=expression_df_with_CommunityAssignment\n",
    "\n",
    "    last_row = df.iloc[-1, :]\n",
    "\n",
    "    row_sums = df.iloc[:-1, :].sum(axis=1)\n",
    "    col_sums = df.iloc[:-1, :].sum(axis=0)\n",
    "\n",
    "    filtered_genes = row_sums > thres\n",
    "    filtered_df = df.iloc[:-1, :].loc[filtered_genes, :]\n",
    "\n",
    "    filtered_cells = col_sums > thres\n",
    "    filtered_df = filtered_df.loc[:, filtered_cells]\n",
    "    last_row_filtered=last_row.loc[filtered_cells]\n",
    "    filtered_df = filtered_df.append(last_row_filtered)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "\n",
    "def generate_custom_colormap(num_colors):\n",
    "    \"\"\"\n",
    "    Generate a custom colormap with a specified number of colors by combining multiple Sequential Color Brewer palettes.\n",
    "    \"\"\"\n",
    "    colormaps = ['RdPu', 'Purples_d', 'flare','Wistia','Greens_d','Oranges_d','Blues','gray']  \n",
    "    \n",
    "    \n",
    "    #colormaps = ['Spectral'] \n",
    "    n_maps=len(colormaps)\n",
    "    avg_c_per_palette=int(np.floor(num_colors/n_maps))\n",
    "    residual=num_colors%n_maps\n",
    "    n_c_per_palette=[avg_c_per_palette for i in range(n_maps)]\n",
    "    \n",
    "    for i,j in enumerate(range(residual)):\n",
    "        n_c_per_palette[i]+=1\n",
    "        \n",
    "    \n",
    "    color_list = []\n",
    "\n",
    "    while len(color_list) < num_colors:\n",
    "        for i,cmap in enumerate(colormaps):\n",
    "            colors = sns.color_palette(cmap, n_colors=n_c_per_palette[i])\n",
    "            color_list.extend(colors)\n",
    "\n",
    "    color_list = color_list[:num_colors]\n",
    "\n",
    "    return color_list\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9fc284a-71e3-4d31-91d1-d2fd238b828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>377</th>\n",
       "      <th>2</th>\n",
       "      <th>37</th>\n",
       "      <th>211</th>\n",
       "      <th>114</th>\n",
       "      <th>163</th>\n",
       "      <th>549</th>\n",
       "      <th>726</th>\n",
       "      <th>723</th>\n",
       "      <th>341</th>\n",
       "      <th>...</th>\n",
       "      <th>708</th>\n",
       "      <th>845</th>\n",
       "      <th>681</th>\n",
       "      <th>685</th>\n",
       "      <th>116</th>\n",
       "      <th>566</th>\n",
       "      <th>435</th>\n",
       "      <th>306</th>\n",
       "      <th>507</th>\n",
       "      <th>738</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.124375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.678178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.998859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.941175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.163541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.980879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.871966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13.756985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.4632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.256263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>community</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3001 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           377        2    37   211  114        163  549        726  723  \\\n",
       "0          0.0        0.0  0.0  0.0  0.0  13.124375  0.0        0.0  0.0   \n",
       "1          0.0        0.0  0.0  0.0  0.0  12.998859  0.0        0.0  0.0   \n",
       "2          0.0        0.0  0.0  0.0  0.0        0.0  0.0  13.163541  0.0   \n",
       "3          0.0  13.756985  0.0  0.0  0.0        0.0  0.0    12.4632  0.0   \n",
       "4          0.0        0.0  0.0  0.0  0.0        0.0  0.0        0.0  0.0   \n",
       "...        ...        ...  ...  ...  ...        ...  ...        ...  ...   \n",
       "2996       0.0        0.0  0.0  0.0  0.0        0.0  0.0        0.0  0.0   \n",
       "2997       0.0        0.0  0.0  0.0  0.0        0.0  0.0        0.0  0.0   \n",
       "2998       0.0        0.0  0.0  0.0  0.0        0.0  0.0        0.0  0.0   \n",
       "2999       0.0        0.0  0.0  0.0  0.0        0.0  0.0        0.0  0.0   \n",
       "community   11          0    1    6    3          4   16         21   21   \n",
       "\n",
       "                 341  ...  708  845  681        685        116  566  \\\n",
       "0                0.0  ...  0.0  0.0  0.0  12.678178        0.0  0.0   \n",
       "1                0.0  ...  0.0  0.0  0.0  12.941175        0.0  0.0   \n",
       "2          12.980879  ...  0.0  0.0  0.0        0.0        0.0  0.0   \n",
       "3                0.0  ...  0.0  0.0  0.0        0.0        0.0  0.0   \n",
       "4                0.0  ...  0.0  0.0  0.0        0.0  13.256263  0.0   \n",
       "...              ...  ...  ...  ...  ...        ...        ...  ...   \n",
       "2996             0.0  ...  0.0  0.0  0.0        0.0        0.0  0.0   \n",
       "2997             0.0  ...  0.0  0.0  0.0        0.0        0.0  0.0   \n",
       "2998             0.0  ...  0.0  0.0  0.0        0.0        0.0  0.0   \n",
       "2999             0.0  ...  0.0  0.0  0.0        0.0        0.0  0.0   \n",
       "community         10  ...   21   25   20         20          3   17   \n",
       "\n",
       "                 435  306  507  738  \n",
       "0                0.0  0.0  0.0  0.0  \n",
       "1                0.0  0.0  0.0  0.0  \n",
       "2          12.871966  0.0  0.0  0.0  \n",
       "3                0.0  0.0  0.0  0.0  \n",
       "4                0.0  0.0  0.0  0.0  \n",
       "...              ...  ...  ...  ...  \n",
       "2996             0.0  0.0  0.0  0.0  \n",
       "2997             0.0  0.0  0.0  0.0  \n",
       "2998             0.0  0.0  0.0  0.0  \n",
       "2999             0.0  0.0  0.0  0.0  \n",
       "community         13    9   15   22  \n",
       "\n",
       "[3001 rows x 1000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_clusters = num_modules \n",
    "num_colors=num_clusters\n",
    "color_palette = generate_custom_colormap(num_colors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "non_zero_df = filter_zero_expression_community(shuffled_df_community)\n",
    "non_zero_data=np.array(non_zero_df[:-1].values)\n",
    "print(np.shape(non_zero_data))\n",
    "community_assignments=dict(zip(non_zero_df.columns,non_zero_df.loc['community']))\n",
    "normalized_data = normalize_matrix_cpm(non_zero_data).astype(float)\n",
    "\n",
    "\n",
    "logtransform=True\n",
    "\n",
    "\n",
    "if logtransform==True:\n",
    "    preprocessed_data=np.log2(normalized_data+1)\n",
    "\n",
    "else:\n",
    "    preprocessed_data=normalized_data\n",
    "\n",
    "preprocessed_df=non_zero_df.copy()\n",
    "nrows,ncols=preprocessed_df.shape\n",
    "preprocessed_df.iloc[:-1,:ncols]=preprocessed_data\n",
    "node_list=list(preprocessed_df.columns)\n",
    "\n",
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c71dfdf0-85d7-48ad-8e14-3c05d454a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Simulation_functions\n",
    "import importlib\n",
    "\n",
    "importlib.reload(Simulation_functions)\n",
    "\n",
    "from Simulation_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "405e0329-977c-4f92-8c34-cd83e31d8a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n"
     ]
    }
   ],
   "source": [
    "correlation_threshold_percentile=99\n",
    "\n",
    "# Using Pearson correlation (default)\n",
    "cell_coexpression_weighted_HT_pearson, cell_coexpression_weighted_pearson = create_correlation_network(preprocessed_data, threshold_percentile=99, method='pearson')\n",
    "gene_coexpression_weighted_HT_pearson, gene_coexpression_weighted_pearson = create_correlation_network(preprocessed_data.T, threshold_percentile=99, method='pearson')\n",
    "\n",
    "# Using Spearman correlation\n",
    "cell_coexpression_weighted_HT_spearman, cell_coexpression_weighted_spearman = create_correlation_network(preprocessed_data, threshold_percentile=99, method='spearman')\n",
    "gene_coexpression_weighted_HT_spearman, gene_coexpression_weighted_spearman = create_correlation_network(preprocessed_data.T, threshold_percentile=99, method='spearman')\n",
    "\n",
    "# Using Cosine similarity\n",
    "cell_coexpression_weighted_HT_cosine, cell_coexpression_weighted_cosine = create_correlation_network(preprocessed_data, threshold_percentile=99, method='cosine')\n",
    "gene_coexpression_weighted_HT_cosine, gene_coexpression_weighted_cosine = create_correlation_network(preprocessed_data.T, threshold_percentile=99, method='cosine')\n",
    "\n",
    "\n",
    "# Using Kendall's tau \n",
    "cell_coexpression_weighted_HT_kendall, cell_coexpression_weighted_kendall = create_correlation_network(preprocessed_data, threshold_percentile=99, method='kendall')\n",
    "gene_coexpression_weighted_HT_kendall, gene_coexpression_weighted_kendall = create_correlation_network(preprocessed_data.T, threshold_percentile=99, method='kendall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "500c5339-720d-4439-9426-4686b3266149",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_coexpression_weighted_HT_kendall, cell_coexpression_weighted_kendall = create_correlation_network(preprocessed_data, threshold_percentile=99, method='kendall')\n",
    "gene_coexpression_weighted_HT_kendall, gene_coexpression_weighted_kendall = create_correlation_network(preprocessed_data.T, threshold_percentile=99, method='kendall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd189be1-6ff0-46f5-837d-7c2d481243f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_correlation_network(matrix, threshold_percentile=99, method='pearson'):\n",
    "    # assume matrix columns are the cells\n",
    "    if method == 'pearson':\n",
    "        cell_coexpression = abs(np.corrcoef(matrix.T))\n",
    "    elif method == 'spearman':\n",
    "        corr, _ = spearmanr(matrix)\n",
    "        cell_coexpression = abs(corr)\n",
    "    elif method == 'cosine':\n",
    "        distance_matrix = pdist(matrix.T, metric='cosine')\n",
    "        cell_coexpression = 1 - squareform(distance_matrix)\n",
    "    elif method == 'kendall':\n",
    "        def compute_kendall(i, j):\n",
    "            tau, _ = kendalltau(matrix[:, i], matrix[:, j])\n",
    "            return tau\n",
    "\n",
    "        num_cells = matrix.shape[1]\n",
    "        cell_coexpression = np.zeros((num_cells, num_cells))\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(\n",
    "            delayed(compute_kendall)(i, j) for i in range(num_cells) for j in range(i, num_cells)\n",
    "        )\n",
    "\n",
    "        index = 0\n",
    "        for i in range(num_cells):\n",
    "            for j in range(i, num_cells):\n",
    "                tau = results[index]\n",
    "                cell_coexpression[i, j] = abs(tau)\n",
    "                cell_coexpression[j, i] = abs(tau)\n",
    "                index += 1\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported correlation method: {method}\")\n",
    "\n",
    "    cell_coexpression_flat = cell_coexpression.flatten()\n",
    "    cell_coexpression_weighted = deepcopy(cell_coexpression)\n",
    "\n",
    "    threshold = np.percentile(cell_coexpression_flat, threshold_percentile)\n",
    "    cell_coexpression[cell_coexpression < threshold] = 0\n",
    "    cell_coexpression_filtered = cell_coexpression\n",
    "\n",
    "    return cell_coexpression_filtered, cell_coexpression_weighted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5eb68-8496-4630-b5a7-cece5abcc06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02cc03-2eca-4415-adc9-6e349db734e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a101df4-8389-4a15-86f0-9540aa82a065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "True\n",
      "Density of the weighted graph after HT: 0.011011011011011011\n",
      "Density of the weighted graph without HT: 1.002002002002002\n",
      "Minimum non-zero edge weight: 0.07222417202946398\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1000000000.0\n",
      "1000000000.0\n",
      "[(377, 377, 1.0), (377, 391, 0.08719888646022314), (377, 388, 0.09158751607618193), (377, 191, 0.08306061728970689), (377, 786, 0.08055454725264583), (2, 2, 1.0), (2, 164, 0.08874499765079011), (2, 908, 0.07652186603129553), (2, 4, 0.08900075465282867), (2, 902, 0.07865847405179022)]\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1715290.8187440804\n",
      "1715290.8187440804\n",
      "(3000, 1000)\n",
      "(3000, 1000)\n",
      "Adjusted Rand Index: 0.47186920746862676\n",
      "Normalized Mutual Information: 0.7614719739778023\n",
      "Adjusted Rand Index: -0.002373998344705438\n",
      "Normalized Mutual Information: 0.11360564548385516\n",
      "25\n",
      "Adjusted Rand Index: 0.20341890758296238\n",
      "Normalized Mutual Information: 0.5221961327541592\n",
      "community_detection_weightedHT\n",
      "Adjusted Rand Index: 0.43806168569679826\n",
      "Normalized Mutual Information: 0.6593057391781157\n",
      "1\n",
      "Adjusted Rand Index: 0.0\n",
      "Normalized Mutual Information: 0.0\n",
      "78\n",
      "Adjusted Rand Index: 0.03284154800070047\n",
      "Normalized Mutual Information: 0.3122531707887585\n",
      "9\n",
      "Adjusted Rand Index: -8.913852832275398e-05\n",
      "Normalized Mutual Information: 0.04295318282554279\n",
      "28\n",
      "Adjusted Rand Index: 0.0025779435826264797\n",
      "Normalized Mutual Information: 0.08222045499019512\n",
      "15\n",
      "Adjusted Rand Index: -0.004324347978821732\n",
      "Normalized Mutual Information: 0.06001316604243872\n",
      "80\n",
      "Adjusted Rand Index: 0.034479581703395414\n",
      "Normalized Mutual Information: 0.3249821708088631\n",
      "1000\n",
      "True\n",
      "Density of the weighted graph after HT: 0.011011011011011011\n",
      "Density of the weighted graph without HT: 0.9997697697697697\n",
      "Minimum non-zero edge weight: 0.11580806681528344\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1000000000.0\n",
      "1000000000.0\n",
      "[(377, 377, 1.0), (377, 391, 0.12628734592715107), (377, 388, 0.1302696083266791), (377, 191, 0.1273472440507879), (377, 786, 0.1227956089656187), (2, 2, 1.0), (2, 164, 0.13218685565615762), (2, 908, 0.11953794689334485), (2, 4, 0.13666613308846953), (2, 902, 0.12416731542416182)]\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1715290.8187440804\n",
      "1715290.8187440804\n",
      "(3000, 1000)\n",
      "(3000, 1000)\n",
      "Adjusted Rand Index: 0.2864420033724595\n",
      "Normalized Mutual Information: 0.6238544610626956\n",
      "Adjusted Rand Index: -0.0036399561756412786\n",
      "Normalized Mutual Information: 0.12445489870624388\n",
      "28\n",
      "Adjusted Rand Index: 0.11907222168283553\n",
      "Normalized Mutual Information: 0.40691088130934416\n",
      "community_detection_weightedHT\n",
      "Adjusted Rand Index: 0.44958535295263025\n",
      "Normalized Mutual Information: 0.6601179653847667\n",
      "1\n",
      "Adjusted Rand Index: 0.0\n",
      "Normalized Mutual Information: 0.0\n",
      "82\n",
      "Adjusted Rand Index: 0.033473637421969925\n",
      "Normalized Mutual Information: 0.30902488819812096\n",
      "4\n",
      "Adjusted Rand Index: -0.0007361903256829245\n",
      "Normalized Mutual Information: 0.016743852368676374\n",
      "32\n",
      "Adjusted Rand Index: 0.00045517350499106715\n",
      "Normalized Mutual Information: 0.06021722912219071\n",
      "9\n",
      "Adjusted Rand Index: -0.002935579357223841\n",
      "Normalized Mutual Information: 0.036757487816870556\n",
      "70\n",
      "Adjusted Rand Index: 0.03376531783821069\n",
      "Normalized Mutual Information: 0.28923447613585546\n",
      "1000\n",
      "True\n",
      "Density of the weighted graph after HT: 0.011011011011011011\n",
      "Density of the weighted graph without HT: 1.002002002002002\n",
      "Minimum non-zero edge weight: 0.07307715801841883\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1000000000.0\n",
      "1000000000.0\n",
      "[(377, 377, 0.9999999999999998), (377, 384, 0.0732278896457694), (377, 391, 0.08758213805525039), (377, 388, 0.09419085459037817), (377, 191, 0.08374514046664427), (377, 786, 0.08041349181091778), (2, 2, 1.0), (2, 164, 0.09015411313912776), (2, 908, 0.07590109811170902), (2, 4, 0.09404539827710902)]\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1715290.8187440804\n",
      "1715290.8187440804\n",
      "(3000, 1000)\n",
      "(3000, 1000)\n",
      "Adjusted Rand Index: 0.4727750071900171\n",
      "Normalized Mutual Information: 0.7716173159450719\n",
      "Adjusted Rand Index: -0.0037905371791920345\n",
      "Normalized Mutual Information: 0.11109010371979942\n",
      "26\n",
      "Adjusted Rand Index: 0.1911110063029596\n",
      "Normalized Mutual Information: 0.5191813241613009\n",
      "community_detection_weightedHT\n",
      "Adjusted Rand Index: 0.495061782016213\n",
      "Normalized Mutual Information: 0.6897950657753437\n",
      "1\n",
      "Adjusted Rand Index: 0.0\n",
      "Normalized Mutual Information: 0.0\n",
      "80\n",
      "Adjusted Rand Index: 0.03418151509382386\n",
      "Normalized Mutual Information: 0.32205193990574277\n",
      "9\n",
      "Adjusted Rand Index: -0.0010597919335514378\n",
      "Normalized Mutual Information: 0.041703899684092126\n",
      "29\n",
      "Adjusted Rand Index: 0.0022361851897804144\n",
      "Normalized Mutual Information: 0.09676450222360404\n",
      "16\n",
      "Adjusted Rand Index: -0.0013532989392165686\n",
      "Normalized Mutual Information: 0.07785167528590184\n",
      "71\n",
      "Adjusted Rand Index: 0.0319184233940888\n",
      "Normalized Mutual Information: 0.3049555838534373\n",
      "1000\n",
      "True\n",
      "Density of the weighted graph after HT: 0.011011011011011011\n",
      "Density of the weighted graph without HT: 1.0019459459459459\n",
      "Minimum non-zero edge weight: 0.07073785950402346\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1000000000.0\n",
      "1000000000.0\n",
      "[(377, 377, 1.0), (377, 391, 0.08557644296818717), (377, 388, 0.0900271476873243), (377, 191, 0.08123627103148567), (377, 786, 0.07881113856477387), (2, 2, 1.0), (2, 164, 0.0868929703426422), (2, 908, 0.07487072182004376), (2, 4, 0.08719627254975096), (2, 902, 0.07695696762715959)]\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1715290.8187440804\n",
      "1715290.8187440804\n",
      "(3000, 1000)\n",
      "(3000, 1000)\n",
      "Adjusted Rand Index: 0.4566381592045763\n",
      "Normalized Mutual Information: 0.7510810270047337\n",
      "Adjusted Rand Index: -0.0031166754827495952\n",
      "Normalized Mutual Information: 0.11671503379563748\n",
      "27\n",
      "Adjusted Rand Index: 0.20921712223468467\n",
      "Normalized Mutual Information: 0.5236198332638917\n",
      "community_detection_weightedHT\n",
      "Adjusted Rand Index: 0.4269637147911184\n",
      "Normalized Mutual Information: 0.6547291182057472\n",
      "1\n",
      "Adjusted Rand Index: 0.0\n",
      "Normalized Mutual Information: 0.0\n",
      "80\n",
      "Adjusted Rand Index: 0.03507466909177557\n",
      "Normalized Mutual Information: 0.32209635229665345\n",
      "10\n",
      "Adjusted Rand Index: -0.000995094433779003\n",
      "Normalized Mutual Information: 0.047380327461254194\n",
      "28\n",
      "Adjusted Rand Index: 0.004909614161164627\n",
      "Normalized Mutual Information: 0.09050163847703081\n",
      "13\n",
      "Adjusted Rand Index: -0.0018913016909930744\n",
      "Normalized Mutual Information: 0.0619562394765323\n",
      "74\n",
      "Adjusted Rand Index: 0.034930775252597274\n",
      "Normalized Mutual Information: 0.31931522620301384\n"
     ]
    }
   ],
   "source": [
    "for correlation_computation_method in ['spearman','cosine','pearson','kendall']:\n",
    "#for correlation_computation_method in ['kendall']:\n",
    "\n",
    "    \n",
    "    exec(f'G_cell_weighted = nx.from_numpy_array(cell_coexpression_weighted_{correlation_computation_method})')\n",
    "    exec(f'G_cell_weighted_HT = nx.from_numpy_array(cell_coexpression_weighted_HT_{correlation_computation_method})')\n",
    "    exec(f'clustering_performance_{correlation_computation_method}=defaultdict()')\n",
    "\n",
    "    cell_map={i:j for i,j in enumerate(list(preprocessed_df.columns))}\n",
    "    G_cell_weighted = nx.relabel_nodes(G_cell_weighted, cell_map)\n",
    "    G_cell_weighted_HT = nx.relabel_nodes(G_cell_weighted_HT, cell_map)\n",
    "\n",
    "    print(len(G_cell_weighted.nodes))\n",
    "\n",
    "    print(list(G_cell_weighted.nodes)==node_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    density = nx.density(G_cell_weighted_HT)\n",
    "    print(\"Density of the weighted graph after HT:\", density)\n",
    "\n",
    "    density = nx.density(G_cell_weighted)\n",
    "    print(\"Density of the weighted graph without HT:\", density)\n",
    "\n",
    "    non_zero_weights = [data['weight'] for _, _, data in G_cell_weighted_HT.edges(data=True) if data['weight'] > 0]\n",
    "\n",
    "    if non_zero_weights:  \n",
    "        min_weight = min(non_zero_weights)\n",
    "        print(\"Minimum non-zero edge weight:\", min_weight)\n",
    "\n",
    "    print(np.sum(np.sum(non_zero_data,axis=0)==1))\n",
    "    print(np.sum(np.sum(non_zero_data,axis=0)==0))\n",
    "    print(np.sum(np.sum(normalized_data,axis=0)==1))\n",
    "    print(np.sum(np.sum(normalized_data,axis=0)==0))\n",
    "    print(np.sum(np.sum(normalized_data,axis=0)))\n",
    "    print(np.sum(np.sum(normalized_data,axis=0)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Define the size threshold\n",
    "    size_threshold = 5\n",
    "\n",
    "    community_sizes = Counter(community_assignments.values())\n",
    "\n",
    "    filtered_communities = {node: community_id for node, community_id in community_assignments.items() if community_sizes[community_id] >= size_threshold}\n",
    "\n",
    "    organized_communities = defaultdict(list)\n",
    "\n",
    "    for node, community_id in filtered_communities.items():\n",
    "        organized_communities[community_id].append(node)\n",
    "\n",
    "    organized_communities = dict(organized_communities)\n",
    "\n",
    "    edges_with_weights_HT = [(u, v, d['weight']) for u, v, d in G_cell_weighted_HT.edges(data=True)]\n",
    "    print(edges_with_weights_HT[:10])\n",
    "\n",
    "\n",
    "    # # correlation after hard thresholding percentile preprocessed_data_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(np.sum(np.sum(non_zero_data,axis=0)==1))\n",
    "    print(np.sum(np.sum(non_zero_data,axis=0)==0))\n",
    "\n",
    "    print(np.sum(np.sum(preprocessed_data,axis=0)==1))\n",
    "    print(np.sum(np.sum(preprocessed_data,axis=0)==0))\n",
    "\n",
    "\n",
    "    print(np.sum(np.sum(preprocessed_data,axis=0)))\n",
    "    print(np.sum(np.sum(preprocessed_data,axis=0)))\n",
    "\n",
    "    data=preprocessed_data\n",
    "\n",
    "\n",
    "\n",
    "    # # correlation network\n",
    "\n",
    "    # # clustering by community detection on correlation networks\n",
    "\n",
    "\n",
    "\n",
    "    print(preprocessed_data.shape)\n",
    "    print(sparse_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    import community\n",
    "    G=G_cell_weighted\n",
    "\n",
    "    partition = community.best_partition(G, weight='weight')\n",
    "\n",
    "\n",
    "\n",
    "    size_threshold = 5\n",
    "\n",
    "    community_sizes = Counter(partition.values())\n",
    "\n",
    "    # Filter communities based on the size threshold\n",
    "    filtered_communities = {node: community_id for node, community_id in partition.items() if community_sizes[community_id] >= size_threshold}\n",
    "\n",
    "    organized_communities = defaultdict(list)\n",
    "    for node, community_id in filtered_communities.items():\n",
    "        organized_communities[community_id].append(node)\n",
    "    organized_communities = dict(organized_communities)\n",
    "\n",
    "\n",
    "\n",
    "    df = preprocessed_df.T.copy()\n",
    "\n",
    "\n",
    "    df['community'] = list(partition.values())\n",
    "\n",
    "    df_sorted = df.sort_values('community')\n",
    "\n",
    "    df_sorted = df_sorted.drop('community', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    community_dict=partition\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "    community_dict=community_assignments\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    true_labels = list(sorted_dict.values())\n",
    "\n",
    "    ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels, average_method='arithmetic')\n",
    "\n",
    "    print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['Louvain_weighted_ARI']=ari_score\")\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['Louvain_weighted_NMI']=nmi_score\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # Girvan Newman doesnt scale well way too slow\n",
    "    # \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #communities = next(girvan_newman(G))\n",
    "    #community_dict = {node: i for i, community in enumerate(communities) for node in community}\n",
    "    #sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    #predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "\n",
    "    # # Leiden\n",
    "\n",
    "\n",
    "\n",
    "    G_ig = ig.Graph.TupleList(G.edges(data=True), weights=True)\n",
    "\n",
    "\n",
    "    partition_leiden = la.find_partition(\n",
    "        G_ig, \n",
    "        la.RBConfigurationVertexPartition,  \n",
    "        weights=[i['weight'] for i in G_ig.es['weight']], resolution_parameter=1.1,n_iterations=-1)\n",
    "\n",
    "    community_dict_leiden = {node: cid for cid, community in enumerate(partition_leiden) for node in community}\n",
    "    sorted_dict = {k: community_dict_leiden[k] for k in node_list}\n",
    "    predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "    community_dict=community_assignments\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    true_labels = list(sorted_dict.values())\n",
    "\n",
    "    ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels, average_method='arithmetic')\n",
    "\n",
    "    print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score}\")\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['Leiden_weighted_ARI']=ari_score\")\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['Leiden_weighted_NMI']=nmi_score\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(len(set(predicted_labels)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = preprocessed_df.T.copy()\n",
    "\n",
    "\n",
    "    df['community'] = list(community_dict_leiden.values())\n",
    "\n",
    "    df_sorted = df.sort_values('community')\n",
    "\n",
    "    df_sorted = df_sorted.drop('community', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # Clauset-Newman-Moore greedy modularity\n",
    "    # https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.modularity_max.greedy_modularity_communities.html\n",
    "    # \n",
    "    # References\n",
    "    # \n",
    "    # [1]\n",
    "    # Newman, M. E. J. “Networks: An Introduction”, page 224 Oxford University Press 2011.\n",
    "    # [2]\n",
    "    # Clauset, A., Newman, M. E., & Moore, C. “Finding community structure in very large networks.” Physical Review E 70(6), 2004.\n",
    "    # [3]\n",
    "    # Reichardt and Bornholdt “Statistical Mechanics of Community Detection” Phys. Rev. E74, 2006.\n",
    "    # [4]\n",
    "    # Newman, M. E. J.”Analysis of weighted networks” Physical Review E 70(5 Pt 2):056131, 2004.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    partition_greedy_modularity = nx.community.greedy_modularity_communities(G,weight='weight')\n",
    "\n",
    "    community_dict_gm={i:n for n, c in enumerate(partition_greedy_modularity) for i in c}\n",
    "\n",
    "\n",
    "    sorted_dict = {k: community_dict_gm[k] for k in node_list}\n",
    "    predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "    community_dict=community_assignments\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    true_labels = list(sorted_dict.values())\n",
    "\n",
    "    ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels, average_method='arithmetic')\n",
    "\n",
    "    print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score}\")\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['GreedyModularity_weighted_ARI']=ari_score\")\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['GreedyModularity_weighted_NMI']=nmi_score\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = preprocessed_df.T.copy()\n",
    "\n",
    "\n",
    "    df['community'] = [community_dict_gm[i] for i in node_list]\n",
    "\n",
    "    df_sorted = df.sort_values('community')\n",
    "\n",
    "    df_sorted = df_sorted.drop('community', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # Community detection HT\n",
    "\n",
    "\n",
    "    # weighted but sparsified by HT\n",
    "    G=G_cell_weighted_HT\n",
    "\n",
    "    partition = community.best_partition(G, weight='weight')\n",
    "    size_threshold = 5\n",
    "    community_sizes = Counter(partition.values())\n",
    "    filtered_communities = {node: community_id for node, community_id in partition.items() if community_sizes[community_id] >= size_threshold}\n",
    "\n",
    "    organized_communities = defaultdict(list)\n",
    "    for node, community_id in filtered_communities.items():\n",
    "        organized_communities[community_id].append(node)\n",
    "    organized_communities = dict(organized_communities)\n",
    "\n",
    "    community_dict=partition\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "    community_dict=community_assignments\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    true_labels = list(sorted_dict.values())\n",
    "\n",
    "    ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels, average_method='arithmetic')\n",
    "    print(\"community_detection_weightedHT\")\n",
    "    print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score}\")\n",
    "\n",
    "\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['Louvain_weightedHT_ARI']=ari_score\")\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['Louvain_weightedHT_NMI']=nmi_score\")\n",
    "\n",
    "\n",
    "    # # Infomap\n",
    "\n",
    "    G=G_cell_weighted\n",
    "    G_ig = ig.Graph.TupleList(G.edges(data=True), weights=True)\n",
    "\n",
    "    partition = G_ig.community_infomap(edge_weights=[i['weight'] for i in G_ig.es['weight']])\n",
    "    community_dict_infomap = {node: cid for cid, community in enumerate(partition) for node in community}\n",
    "    print(len(set(community_dict_infomap.values())))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sorted_dict = {k: community_dict_infomap[k] for k in node_list}\n",
    "    predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "    community_dict=community_assignments\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    true_labels = list(sorted_dict.values())\n",
    "\n",
    "    ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels, average_method='arithmetic')\n",
    "\n",
    "    print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score}\")\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['infomap_weighted_ARI']=ari_score\")\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['infomap_weighted_NMI']=nmi_score\")\n",
    "\n",
    "\n",
    "\n",
    "    G=G_cell_weighted_HT\n",
    "    G_ig = ig.Graph.TupleList(G.edges(data=True), weights=True)\n",
    "\n",
    "    partition = G_ig.community_infomap(edge_weights=[i['weight'] for i in G_ig.es['weight']])\n",
    "    community_dict_infomap = {node: cid for cid, community in enumerate(partition) for node in community}\n",
    "    print(len(set(community_dict_infomap.values())))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sorted_dict = {k: community_dict_infomap[k] for k in node_list}\n",
    "    predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "    community_dict=community_assignments\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    true_labels = list(sorted_dict.values())\n",
    "\n",
    "    ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels, average_method='arithmetic')\n",
    "\n",
    "    print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['infomap_weightedHT_ARI']=ari_score\")\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['infomap_weightedHT_NMI']=nmi_score\")\n",
    "\n",
    "\n",
    "\n",
    "    df = preprocessed_df.T.copy()\n",
    "\n",
    "\n",
    "    df['community'] = list(community_dict_infomap.values())\n",
    "\n",
    "    df_sorted = df.sort_values('community')\n",
    "\n",
    "    df_sorted = df_sorted.drop('community', axis=1)\n",
    "\n",
    "\n",
    "    method='Infomap'\n",
    "\n",
    "    # # leading_eigenvector\n",
    "\n",
    "    G=G_cell_weighted\n",
    "    G_ig = ig.Graph.TupleList(G.edges(data=True), weights=True)\n",
    "\n",
    "    partition = G_ig.community_leading_eigenvector(clusters=num_clusters,weights=[i['weight'] for i in G_ig.es['weight']])\n",
    "    community_dict_eigen = {node: cid for cid, community in enumerate(partition) for node in community}\n",
    "    print(len(set(community_dict_eigen.values())))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sorted_dict = {k: community_dict_eigen[k] for k in node_list}\n",
    "    predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "    community_dict=community_assignments\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    true_labels = list(sorted_dict.values())\n",
    "\n",
    "    ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels, average_method='arithmetic')\n",
    "\n",
    "    print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score}\")\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['eigenvector_weighted_ARI']=ari_score\")\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['eigenvector_weighted_NMI']=nmi_score\")\n",
    "\n",
    "\n",
    "    G=G_cell_weighted_HT\n",
    "    G_ig = ig.Graph.TupleList(G.edges(data=True), weights=True)\n",
    "\n",
    "    partition = G_ig.community_leading_eigenvector(clusters=num_clusters,weights=[i['weight'] for i in G_ig.es['weight']])\n",
    "    community_dict_eigen = {node: cid for cid, community in enumerate(partition) for node in community}\n",
    "    print(len(set(community_dict_eigen.values())))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sorted_dict = {k: community_dict_eigen[k] for k in node_list}\n",
    "    predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "    community_dict=community_assignments\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    true_labels = list(sorted_dict.values())\n",
    "\n",
    "    ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels, average_method='arithmetic')\n",
    "\n",
    "    print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score}\")\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['eigenvector_weightedHT_ARI']=ari_score\")\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['eigenvector_weightedHT_NMI']=nmi_score\")\n",
    "\n",
    "\n",
    "\n",
    "    df = preprocessed_df.T.copy()\n",
    "\n",
    "\n",
    "    df['community'] = list(community_dict_eigen.values())\n",
    "\n",
    "    df_sorted = df.sort_values('community')\n",
    "\n",
    "    df_sorted = df_sorted.drop('community', axis=1)\n",
    "\n",
    "\n",
    "    method='Eigenvector'\n",
    "\n",
    "\n",
    "\n",
    "    # # community_multilevel\n",
    "    G=G_cell_weighted\n",
    "    G_ig = ig.Graph.TupleList(G.edges(data=True), weights=True)\n",
    "\n",
    "    partition = G_ig.community_multilevel(weights=[i['weight'] for i in G_ig.es['weight']], resolution=1)\n",
    "    community_dict_multilevel = {node: cid for cid, community in enumerate(partition) for node in community}\n",
    "    print(len(set(community_dict_multilevel.values())))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sorted_dict = {k: community_dict_multilevel[k] for k in node_list}\n",
    "    predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "    community_dict=community_assignments\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    true_labels = list(sorted_dict.values())\n",
    "\n",
    "    ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels, average_method='arithmetic')\n",
    "\n",
    "    print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score}\")\n",
    "\n",
    "\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['Multilevel_weighted_ARI']=ari_score\")\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['Multilevel_weighted_NMI']=nmi_score\")\n",
    "\n",
    "\n",
    "\n",
    "    G=G_cell_weighted_HT\n",
    "    G_ig = ig.Graph.TupleList(G.edges(data=True), weights=True)\n",
    "\n",
    "    partition = G_ig.community_multilevel(weights=[i['weight'] for i in G_ig.es['weight']], resolution=1)\n",
    "    community_dict_multilevel = {node: cid for cid, community in enumerate(partition) for node in community}\n",
    "    print(len(set(community_dict_multilevel.values())))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sorted_dict = {k: community_dict_multilevel[k] for k in node_list}\n",
    "    predicted_labels = list(sorted_dict.values())\n",
    "\n",
    "    community_dict=community_assignments\n",
    "    sorted_dict = {k: community_dict[k] for k in node_list}\n",
    "    true_labels = list(sorted_dict.values())\n",
    "\n",
    "    ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels, average_method='arithmetic')\n",
    "\n",
    "    print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi_score}\")\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['Multilevel_weightedHT_ARI']=ari_score\")\n",
    "    exec(f\"clustering_performance_{correlation_computation_method}['Multilevel_weightedHT_NMI']=nmi_score\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = preprocessed_df.T.copy()\n",
    "\n",
    "\n",
    "    df['community'] = list(community_dict_multilevel.values())\n",
    "\n",
    "    df_sorted = df.sort_values('community')\n",
    "\n",
    "    df_sorted = df_sorted.drop('community', axis=1)\n",
    "\n",
    "\n",
    "    method='multilevel'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "649f443e-740a-4f27-888d-16a78d70862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,correlation_computation_method in enumerate(['pearson','cosine','spearman','kendall']):\n",
    "    exec(f'dict{i+1}=clustering_performance_{correlation_computation_method}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332c772-aabc-4386-8deb-5758fb728573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4e76f6d-c30d-4c39-ac76-e7136ef1f8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "{} &         Pearson &          Cosine &         Spearman &         Kendall \\\\\n",
      "\\midrule\n",
      "Louvain_weightedHT (ARI)     &           0.495 &           0.450 &            0.438 &  \\textbf{0.427} \\\\\n",
      "infomap_weighted (ARI)       &  \\textbf{0.000} &  \\textbf{0.000} &   \\textbf{0.000} &  \\textbf{0.000} \\\\\n",
      "eigenvector_weightedHT (ARI) &           0.002 &  \\textbf{0.000} &            0.003 &           0.005 \\\\\n",
      "Multilevel_weighted (ARI)    &          -0.001 &          -0.003 &  \\textbf{-0.004} &          -0.002 \\\\\n",
      "Multilevel_weightedHT (ARI)  &  \\textbf{0.032} &           0.034 &            0.034 &           0.035 \\\\\n",
      "Mean (ARI)                   &           0.106 &           0.096 &            0.094 &           0.093 \\\\\n",
      "Louvain_weightedHT (NMI)     &           0.690 &           0.660 &            0.659 &  \\textbf{0.655} \\\\\n",
      "infomap_weighted (NMI)       &  \\textbf{0.000} &  \\textbf{0.000} &   \\textbf{0.000} &  \\textbf{0.000} \\\\\n",
      "eigenvector_weightedHT (NMI) &           0.097 &  \\textbf{0.060} &            0.082 &           0.091 \\\\\n",
      "Multilevel_weighted (NMI)    &           0.078 &  \\textbf{0.037} &            0.060 &           0.062 \\\\\n",
      "Multilevel_weightedHT (NMI)  &           0.305 &  \\textbf{0.289} &            0.325 &           0.319 \\\\\n",
      "Mean (NMI)                   &           0.234 &           0.209 &            0.225 &           0.225 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you've already executed the code to create dict1, dict2, dict3, and dict4\n",
    "\n",
    "# Create a DataFrame from the dictionaries\n",
    "df = pd.DataFrame({\n",
    "    \"Pearson\": dict1,\n",
    "    \"Cosine\": dict2,\n",
    "    \"Spearman\": dict3,\n",
    "    \"Kendall\": dict4\n",
    "})\n",
    "\n",
    "# Rename the index for better readability\n",
    "method_names = {\n",
    "    'Louvain_weighted': 'Louvain',\n",
    "    'Leiden_weighted': 'Leiden',\n",
    "    'GreedyModularity_weighted': 'Greedy Modularity',\n",
    "    'infomap_weightedHT': 'Infomap',\n",
    "    'eigenvector_weighted': 'Eigenvector'\n",
    "}\n",
    "\n",
    "df.index = [method_names.get(idx.rsplit('_', 1)[0], idx) for idx in df.index]\n",
    "\n",
    "# Separate ARI and NMI\n",
    "df_ari = df[df.index.str.endswith('ARI')].copy()\n",
    "df_nmi = df[df.index.str.endswith('NMI')].copy()\n",
    "\n",
    "# Remove the metric suffix from the index\n",
    "df_ari.index = df_ari.index.str.replace('_ARI', '')\n",
    "df_nmi.index = df_nmi.index.str.replace('_NMI', '')\n",
    "\n",
    "# Function to format the table\n",
    "def format_table(df, metric):\n",
    "    # Round to 3 decimal places\n",
    "    df = df.round(3)\n",
    "    \n",
    "    # Add a row for the mean\n",
    "    df.loc['Mean'] = df.mean()\n",
    "    \n",
    "    # Format as strings with 3 decimal places\n",
    "    df = df.applymap(lambda x: f\"{x:.3f}\")\n",
    "    \n",
    "    # Bold the lowest value in each row (excluding the mean row)\n",
    "    for idx in df.index[:-1]:\n",
    "        min_val = df.loc[idx].astype(float).min()\n",
    "        df.loc[idx] = df.loc[idx].apply(lambda x: f\"\\\\textbf{{{x}}}\" if float(x) == min_val else x)\n",
    "    \n",
    "    # Add the metric to the index name\n",
    "    df.index = [f\"{idx} ({metric})\" for idx in df.index]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Format ARI and NMI tables\n",
    "df_ari_formatted = format_table(df_ari, \"ARI\")\n",
    "df_nmi_formatted = format_table(df_nmi, \"NMI\")\n",
    "\n",
    "# Combine the formatted tables\n",
    "df_combined = pd.concat([df_ari_formatted, df_nmi_formatted])\n",
    "\n",
    "# Generate LaTeX code\n",
    "latex_code = df_combined.to_latex(escape=False)\n",
    "\n",
    "print(latex_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
